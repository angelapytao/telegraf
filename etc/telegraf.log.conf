#agent配置
[agent]
# #默认的数据(input)采集间隔时间
interval = "10s"
# 采用轮询时间间隔。默认是使用interval里面的值进行轮询，比如interval = "10s",那采集时间将是:00, :10, :20, 等
round_interval = true
# 每次发送到output的度量大小不能超过metric_batch_size的值
metric_batch_size = 1000
# telegraf会为每一个output去缓存一份度量值，metric_buffer_limit为缓存的限制，并且刷新buffer以确定成功写入。如果达到这个限制了，老的数据会被第一时间丢弃
# 当然了，增加这个值能够容忍更多的数据连接，但是这也将会增加telegraf潜在的内存占用。这个值可以大于metric_batch_size但是必须小于它的两倍
metric_buffer_limit = 10000
# 通过随机度量来对采集时间进行抖动。每个插件在采集数据之前将会有一个随机时间的休眠，但是这个时间应小于collection_jitter
# 这个设置是为了防止多个采集源数据同一时间都在队列
collection_jitter = "0s"
# 默认所有数据flush到outputs的时间(在数据被flush到output之前，最大能到flush_interval + flush_jitter)。不能低于interval
flush_interval = "10s"
# 通过随机数来对flush间隔进行抖动。这个主要是为了避免当运行一个大的telegraf实例的时候有比较大的写入。(jitter=5s,flush_interval=10s意味着每10-15s会发生一次flush操作)
flush_jitter = "5s"
#默认这个值被设置相同的时间戳通过采集间隔排序。最大值为1s。这个指标一般不会用在service input(比如logparser和statsd)。单位(ns,us,ms,s)
precision = "s"
#以debug模式运行
debug = false
#以安静模式运行
quiet = false
#这个将会覆盖默认的hostname，如果为空的话，将会采用os.Hostname()
hostname = "test-mac"
#如果设置为true，就不允许在telegraf agent里面设置"host"标签了
omit_hostname = false
# 如果设置为true, 自动获取本机IP地址作为 hostname
use_localIp_as_host = true
[[inputs.log]] 
 files = ["/home/logs/app-*.log"] #日志文件路径匹配规则
 from_beginning = false #是否从日志文件开头处读取日志
 pipe = false #是否开启命名管道
 watch_method = "poll" #监控日志方式
 max_undelivered_lines = 1000 #发送前最大日志队列集合
 regexp = "^\\[.*" #提取日志行的正则表达式
  
[inputs.log.multiline]
  pattern = "^(\\s|Caused by|,|')" #匹配多行日志的正则表达式（以这个开头的日志行将合并成一个字符串）
  match_which_line = "previous" #多行日志监控方式（从队列前还是队列末尾）

[[inputs.log]] 
 files = ["/home/logs/err-*.log"] #日志文件路径匹配规则
 from_beginning = false #是否从日志文件开头处读取日志
 pipe = false #是否开启命名管道
 watch_method = "poll" #监控日志方式
 max_undelivered_lines = 1000 #发送前最大日志队列集合
 regexp = "^\\[.*" #提取日志行的正则表达式
  
[inputs.log.multiline]
  pattern = "^(\\s|Caused by|,|')" #匹配多行日志的正则表达式（以这个开头的日志行将合并成一个字符串）
  match_which_line = "previous" #多行日志监控方式（从队列前还是队列末尾）
 
[[processors.script]]
  source = '''
	  function process(event) {
	  if (event.Get("log") !== "") {
	         var path = event.Get("log.file.path");
	         var arr = path.split("/");
	         var logName = arr[arr.length-1];
	         var appName = logName.replace(/\.log.*/,"").replace("trace-","");
	         appName = appName.replace(/[\-|\.]202\d{1}\-\d{2}\-\d{2}(\-\d{0,10})?/, "");
	         event.Put("topic", "trace-log-"+appName);
	         event.Put("fields.evn", "dev");
	         var logOffset = event.Get("log.offset");
	         if (logOffset !== 0) {
	                 event.Put("offset", logOffset);
	         }
	     }
	  }
 '''

#[[outputs.file]]
#  files = ["stdout"]
#  data_format = "json"

[[outputs.kafka2]] #日志插件输出类型
  namepass = ["log"]  #输出插件的名称过滤
  data_format = "json2" #输出格式
  flush_interval = "500ms" #日志发送间隔
  metric_batch_size = 1000 #日志队列的最大长度（达到这个长度将立即发送）
  show_send = true
  brokers = ["172.16.10.183:9092","172.16.10.195:9092","172.16.10.196:9092"] #日志服务器地址
